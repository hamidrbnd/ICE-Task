{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fcb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c923ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hr_ba\\AppData\\Local\\Temp\\ipykernel_26944\\1448315318.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_book = pd.read_csv('Books.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book = pd.read_csv('Books.csv')\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21eaef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16807,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book['Publisher'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55bd8145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0   276725  034545104X            0\n",
       "1   276726  0155061224            5\n",
       "2   276727  0446520802            0\n",
       "3   276729  052165615X            3\n",
       "4   276729  0521795028            6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rating = pd.read_csv('Ratings.csv')\n",
    "df_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f687d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user = pd.read_csv('Users.csv')\n",
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab76e3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(110762)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user['Age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195d283e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278858, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8ff339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location\n",
       "3    277348\n",
       "4      1417\n",
       "5        72\n",
       "6        11\n",
       "7         4\n",
       "9         2\n",
       "8         2\n",
       "2         1\n",
       "1         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user['Location'].apply(lambda x: len(x.split(','))).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ec6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Imports & config\n",
    "import os, re, math, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = \".\"                  # where Books.csv, Users.csv, Ratings.csv live\n",
    "OUT_DIR  = \"./processed\"        # where we’ll save cleaned files\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1) Load raw CSVs (be tolerant to mixed types)\n",
    "books   = pd.read_csv(f\"{DATA_DIR}/Books.csv\", dtype=str, encoding=\"latin-1\")\n",
    "ratings = pd.read_csv(f\"{DATA_DIR}/Ratings.csv\", dtype=str, encoding=\"latin-1\")\n",
    "users   = pd.read_csv(f\"{DATA_DIR}/Users.csv\", dtype=str, encoding=\"latin-1\")\n",
    "\n",
    "# Standardize column names\n",
    "books.columns   = [c.strip().replace(\" \", \"-\") for c in books.columns]\n",
    "ratings.columns = [c.strip().replace(\" \", \"-\") for c in ratings.columns]\n",
    "users.columns   = [c.strip().replace(\" \", \"-\") for c in users.columns]\n",
    "\n",
    "# --- 2) Light cleaning helpers\n",
    "def to_int_safe(x):\n",
    "    try:\n",
    "        return int(float(str(x).strip()))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def clamp_year(y):\n",
    "    y = to_int_safe(y)\n",
    "    # reasonable publication years (Book-Crossing has oddities)\n",
    "    return y if (not pd.isna(y) and 1450 <= y <= 2025) else np.nan\n",
    "\n",
    "def clean_age(a):\n",
    "    a = to_int_safe(a)\n",
    "    return a if (not pd.isna(a) and 5 <= a <= 95) else np.nan\n",
    "\n",
    "def split_location(loc):\n",
    "    # \"city, state, country\" -> (city, state, country)\n",
    "    if pd.isna(loc): return pd.Series({\"City\": np.nan, \"State\": np.nan, \"Country\": np.nan})\n",
    "    parts = [p.strip().lower() for p in str(loc).split(\",\")]\n",
    "    parts += [np.nan] * (3 - len(parts))\n",
    "    return pd.Series({\"City\": parts[0], \"State\": parts[1], \"Country\": parts[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) Clean Users\n",
    "users[\"Age\"] = users[\"Age\"].apply(clean_age)\n",
    "loc_split = users[\"Location\"].apply(split_location)\n",
    "users = pd.concat([users.drop(columns=[\"Location\"], errors=\"ignore\"), loc_split], axis=1)\n",
    "\n",
    "# Age buckets (categorical)\n",
    "bins = [0, 18, 25, 35, 50, 70, 120]\n",
    "labels = [\"<18\",\"18-24\",\"25-34\",\"35-49\",\"50-69\",\"70+\"]\n",
    "users[\"AgeBucket\"] = pd.cut(users[\"Age\"], bins=bins, labels=labels, include_lowest=True)\n",
    "users[\"AgeBucket\"] = users[\"AgeBucket\"].astype(\"category\")\n",
    "users[\"User-ID\"] = users[\"User-ID\"].astype(int)\n",
    "\n",
    "# --- 4) Clean Books\n",
    "books[\"Year-Of-Publication\"] = books[\"Year-Of-Publication\"].apply(clamp_year)\n",
    "# Normalize text\n",
    "for c in [\"Book-Title\", \"Book-Author\", \"Publisher\"]:\n",
    "    if c in books.columns:\n",
    "        books[c] = books[c].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# Keep a de-duplicated ISBN (first occurrence)\n",
    "books = books.drop_duplicates(subset=[\"ISBN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34c421e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',\n",
       "       'Image-URL-S', 'Image-URL-M', 'Image-URL-L'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c432e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Clean Ratings\n",
    "# Cast numeric columns\n",
    "ratings[\"Book-Rating\"] = ratings[\"Book-Rating\"].apply(to_int_safe)\n",
    "ratings[\"User-ID\"]     = ratings[\"User-ID\"].apply(to_int_safe)\n",
    "# Drop rows with missing ids or ratings\n",
    "ratings = ratings.dropna(subset=[\"User-ID\", \"ISBN\", \"Book-Rating\"])\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].astype(int)\n",
    "\n",
    "# Merge metadata (helps filtering invalid ISBNs, etc.)\n",
    "df = ratings.merge(books[[\"ISBN\"]], on=\"ISBN\", how=\"inner\")\n",
    "df = df.merge(users[[\"User-ID\"]], on=\"User-ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d869c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Activity filtering (stabilizes training)\n",
    "min_user_interactions = 5\n",
    "min_item_interactions = 5\n",
    "user_counts = df[\"User-ID\"].value_counts()\n",
    "item_counts = df[\"ISBN\"].value_counts()\n",
    "keep_users = set(user_counts[user_counts >= min_user_interactions].index)\n",
    "keep_items = set(item_counts[item_counts >= min_item_interactions].index)\n",
    "df = df[df[\"User-ID\"].isin(keep_users) & df[\"ISBN\"].isin(keep_items)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Build explicit & implicit views\n",
    "# explicit rating in [0..10] (many zeros in Book-Crossing = \"no explicit rating\")\n",
    "explicit = df.copy()\n",
    "\n",
    "# implicit: 1 if rating >= 5 (you can tune threshold), else 0\n",
    "implicit = df.copy()\n",
    "implicit[\"y\"] = (implicit[\"Book-Rating\"].fillna(0) >= 5).astype(int)\n",
    "\n",
    "# --- 8) Create ID <-> index maps (contiguous indices for modeling)\n",
    "uid2ix = {u:i for i, u in enumerate(sorted(explicit[\"User-ID\"].unique()))}\n",
    "ix2uid = {i:u for u,i in uid2ix.items()}\n",
    "isbn2ix = {b:i for i, b in enumerate(sorted(explicit[\"ISBN\"].unique()))}\n",
    "ix2isbn = {i:b for b,i in isbn2ix.items()}\n",
    "\n",
    "def map_ids(_df):\n",
    "    out = _df.copy()\n",
    "    out[\"uix\"] = out[\"User-ID\"].map(uid2ix)\n",
    "    out[\"iix\"] = out[\"ISBN\"].map(isbn2ix)\n",
    "    return out.dropna(subset=[\"uix\",\"iix\"])\n",
    "\n",
    "explicit_mapped = map_ids(explicit)\n",
    "implicit_mapped = map_ids(implicit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8861c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9) Title text → TF-IDF → SVD(=dense 64-D) as lightweight item content features\n",
    "#     (This helps content/hybrid models + cold-start for items)\n",
    "title_series = books.set_index(\"ISBN\").reindex(sorted(isbn2ix.keys()))[\"Book-Title\"].fillna(\"\").astype(str)\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)\n",
    "X_tfidf = tfidf.fit_transform(title_series.tolist())\n",
    "\n",
    "svd = TruncatedSVD(n_components=64, random_state=RANDOM_SEED)\n",
    "item_title_emb = svd.fit_transform(X_tfidf)  # shape: [num_items, 64]\n",
    "\n",
    "# Attach to a DataFrame aligned with iix\n",
    "item_content = pd.DataFrame(item_title_emb, columns=[f\"title_svd_{k}\" for k in range(item_title_emb.shape[1])])\n",
    "item_content[\"ISBN\"] = sorted(isbn2ix.keys())\n",
    "item_content[\"iix\"]  = item_content[\"ISBN\"].map(isbn2ix)\n",
    "\n",
    "# --- 10) Simple user demographics features (one-hots for top countries + age buckets)\n",
    "users_small = users[users[\"User-ID\"].isin(uid2ix.keys())].copy()\n",
    "users_small[\"uix\"] = users_small[\"User-ID\"].map(uid2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8c3a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K countries to one-hot; rest = \"other\"\n",
    "K = 15\n",
    "top_countries = users_small[\"Country\"].value_counts().head(K).index.tolist()\n",
    "for c in top_countries:\n",
    "    users_small[f\"country__{c}\"] = (users_small[\"Country\"] == c).astype(int)\n",
    "users_small[\"country__other\"] = (~users_small[\"Country\"].isin(top_countries)).astype(int)\n",
    "\n",
    "# Age bucket one-hots\n",
    "for b in users_small[\"AgeBucket\"].cat.categories:\n",
    "    users_small[f\"age__{b}\"] = (users_small[\"AgeBucket\"] == b).astype(int)\n",
    "\n",
    "user_features = users_small[[\"uix\"] + [c for c in users_small.columns if c.startswith((\"country__\", \"age__\"))]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b0f7eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40906, 66)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ca4548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data prepared and saved to ./processed\n",
      "Shapes: \n",
      "  explicit: (596753, 5) \n",
      "  implicit: (596753, 6) \n",
      "  item_content: (40906, 66) \n",
      "  user_features: (20131, 23)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 11) Train/Val/Test split (leave-one-out per user for ranking)\n",
    "def leave_one_out_split(df_in, rating_col=\"Book-Rating\"):\n",
    "    # Assumes each row is a user-item interaction\n",
    "    df_sorted = df_in.sample(frac=1.0, random_state=RANDOM_SEED)  # shuffle\n",
    "    val_rows, test_rows = [], []\n",
    "    seen_for_val, seen_for_test = set(), set()\n",
    "\n",
    "    # pick 1 item per user for val and 1 for test (if available)\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        u = row[\"User-ID\"]\n",
    "        if u not in seen_for_val:\n",
    "            val_rows.append(row.name); seen_for_val.add(u)\n",
    "        elif u not in seen_for_test:\n",
    "            test_rows.append(row.name); seen_for_test.add(u)\n",
    "\n",
    "    val  = df_in.loc[val_rows]\n",
    "    test = df_in.loc[test_rows]\n",
    "    train = df_in.drop(index=set(val_rows) | set(test_rows))\n",
    "    return train, val, test\n",
    "\n",
    "train_exp, val_exp, test_exp = leave_one_out_split(explicit_mapped)\n",
    "train_imp, val_imp, test_imp = leave_one_out_split(implicit_mapped)\n",
    "\n",
    "# --- 12) Save artifacts for next steps\n",
    "books.to_parquet(f\"{OUT_DIR}/books_clean.parquet\", index=False, engine=\"fastparquet\")\n",
    "users.to_parquet(f\"{OUT_DIR}/users_clean.parquet\", index=False, engine=\"fastparquet\")\n",
    "explicit_mapped.to_parquet(f\"{OUT_DIR}/ratings_explicit.parquet\", index=False, engine=\"fastparquet\")\n",
    "implicit_mapped.to_parquet(f\"{OUT_DIR}/ratings_implicit.parquet\", index=False, engine=\"fastparquet\")\n",
    "\n",
    "item_content.to_parquet(f\"{OUT_DIR}/item_title_svd64.parquet\", index=False, engine=\"fastparquet\")\n",
    "user_features.to_parquet(f\"{OUT_DIR}/user_demo_features.parquet\", index=False, engine=\"fastparquet\")\n",
    "\n",
    "\n",
    "pd.Series(uid2ix).to_json(f\"{OUT_DIR}/uid2ix.json\")\n",
    "pd.Series(isbn2ix).to_json(f\"{OUT_DIR}/isbn2ix.json\")\n",
    "\n",
    "print(\"✅ Data prepared and saved to\", OUT_DIR)\n",
    "print(\"Shapes:\",\n",
    "      \"\\n  explicit:\", explicit_mapped.shape,\n",
    "      \"\\n  implicit:\", implicit_mapped.shape,\n",
    "      \"\\n  item_content:\", item_content.shape,\n",
    "      \"\\n  user_features:\", user_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50bb9b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Item recs: ['0373791313', '0373791356', '0515131105', '0609607235', '0671676369']\n",
      "User-User recs: ['0060953691', '0375759778', '0394575202', '0684848783', '0020811853']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['La Cucina: A Novel of Rapture',\n",
       " 'Prague : A Novel',\n",
       " 'The Power of One',\n",
       " 'Tis : A Memoir',\n",
       " 'POSTCARDS']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "# --- Build User-Item sparse matrix (unchanged)\n",
    "n_users = len(uid2ix)\n",
    "n_items = len(isbn2ix)\n",
    "\n",
    "rows = train_imp[\"uix\"].to_numpy()\n",
    "cols = train_imp[\"iix\"].to_numpy()\n",
    "vals = train_imp[\"y\"].to_numpy()\n",
    "\n",
    "ui_matrix = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np, heapq\n",
    "\n",
    "# Fit once (items = columns of UI matrix)\n",
    "knn_items = NearestNeighbors(\n",
    "    n_neighbors=50,       # tune K\n",
    "    metric=\"cosine\",\n",
    "    algorithm=\"brute\",\n",
    "    n_jobs=-1\n",
    ").fit(ui_matrix.T)\n",
    "\n",
    "def recommend_items_for_user(user_id, N=10, K=50):\n",
    "    \"\"\"\n",
    "    On-the-fly item-item recommendations for a user.\n",
    "    Uses only the items the user interacted with, expands to K neighbors each,\n",
    "    and aggregates neighbor similarities.\n",
    "    \"\"\"\n",
    "    if user_id not in uid2ix:\n",
    "        return []\n",
    "\n",
    "    uix = uid2ix[user_id]\n",
    "    user_items = ui_matrix[uix].indices  # items this user interacted with\n",
    "    if len(user_items) == 0:\n",
    "        return []\n",
    "\n",
    "    # Query neighbors for just these items\n",
    "    dist, idx = knn_items.kneighbors(ui_matrix.T[user_items], n_neighbors=K, return_distance=True)\n",
    "    sim = 1.0 - dist   # cosine similarity\n",
    "\n",
    "    # Accumulate scores\n",
    "    scores = np.zeros(n_items, dtype=np.float32)\n",
    "    for nbrs, sims in zip(idx, sim):\n",
    "        # skip self (first neighbor is usually the item itself)\n",
    "        for j, s in zip(nbrs[1:], sims[1:]):\n",
    "            scores[j] += s\n",
    "\n",
    "    # filter items already seen\n",
    "    seen_items = set(user_items.tolist())\n",
    "    if seen_items:\n",
    "        scores[list(seen_items)] = -1e9\n",
    "\n",
    "    top_items = heapq.nlargest(N, range(n_items), key=lambda i: scores[i])\n",
    "    return [ix2isbn[i] for i in top_items]\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np, heapq\n",
    "\n",
    "# Fit once (users = rows of UI matrix)\n",
    "knn_users = NearestNeighbors(\n",
    "    n_neighbors=50,     # tune K\n",
    "    metric=\"cosine\",\n",
    "    algorithm=\"brute\",\n",
    "    n_jobs=-1\n",
    ").fit(ui_matrix)\n",
    "\n",
    "def recommend_by_similar_users(user_id, N=10, K=50):\n",
    "    \"\"\"\n",
    "    User-user collaborative filtering.\n",
    "    Finds K nearest users to the target user and aggregates their interactions\n",
    "    weighted by similarity, all in sparse form (no dense matrix conversion).\n",
    "    \"\"\"\n",
    "    if user_id not in uid2ix:\n",
    "        return []\n",
    "\n",
    "    uix = uid2ix[user_id]\n",
    "\n",
    "    # Find K nearest neighbors for this user (includes self at idx 0)\n",
    "    dist, idx = knn_users.kneighbors(ui_matrix[uix], n_neighbors=K, return_distance=True)\n",
    "    idx = idx.ravel()\n",
    "    sim = (1.0 - dist.ravel()).astype(np.float32)\n",
    "\n",
    "    # drop self if present\n",
    "    mask = idx != uix\n",
    "    nbr_idxs = idx[mask]\n",
    "    nbr_sims = sim[mask]\n",
    "\n",
    "    # Accumulate scores sparsely: scores[j] += w * interaction(nbr, j)\n",
    "    scores = np.zeros(n_items, dtype=np.float32)\n",
    "    for nbr, w in zip(nbr_idxs, nbr_sims):\n",
    "        row = ui_matrix[nbr]           # sparse row\n",
    "        if row.nnz:\n",
    "            scores[row.indices] += w * row.data\n",
    "\n",
    "    # filter items already seen by target user\n",
    "    seen_items = set(ui_matrix[uix].indices.tolist())\n",
    "    if seen_items:\n",
    "        scores[list(seen_items)] = -1e9\n",
    "\n",
    "    top_items = heapq.nlargest(N, range(n_items), key=lambda i: scores[i])\n",
    "    return [ix2isbn[i] for i in top_items]\n",
    "\n",
    "\n",
    "# Pick any known user_id from your mappings\n",
    "some_user_id = next(iter(uid2ix.keys()))\n",
    "\n",
    "dict_book = books[['ISBN', 'Book-Title']].set_index('ISBN').to_dict()['Book-Title']\n",
    "\n",
    "recommended_item_itembased = recommend_items_for_user(some_user_id, N=5, K=50)\n",
    "recommended_item_userbased  = recommend_by_similar_users(some_user_id, N=5, K=50)\n",
    "\n",
    "print(\"Item-Item recs:\", recommended_item_itembased)\n",
    "print(\"User-User recs:\", recommended_item_userbased)\n",
    "[dict_book[x] for x in recommended_item_userbased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def precision_at_k(pred, gt, k):\n",
    "    if k == 0: return 0.0\n",
    "    pred_k = pred[:k]\n",
    "    if len(pred_k) == 0: return 0.0\n",
    "    return len(set(pred_k) & set(gt)) / float(k)\n",
    "\n",
    "def recall_at_k(pred, gt, k):\n",
    "    if len(gt) == 0: return 0.0\n",
    "    pred_k = pred[:k]\n",
    "    return len(set(pred_k) & set(gt)) / float(len(gt))\n",
    "\n",
    "def ndcg_at_k(pred, gt, k):\n",
    "    pred_k = pred[:k]\n",
    "    if len(pred_k) == 0: return 0.0\n",
    "    # relevance = 1 if in GT else 0\n",
    "    gains = [1.0 if p in gt else 0.0 for p in pred_k]\n",
    "    dcg = sum(g / np.log2(i + 2) for i, g in enumerate(gains))\n",
    "    ideal_gains = sorted(gains, reverse=True)\n",
    "    idcg = sum(g / np.log2(i + 2) for i, g in enumerate(ideal_gains))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Build ground-truth sets from TEST split (implicit: any test interaction is \"relevant\")\n",
    "user_test_gt = defaultdict(list)\n",
    "for _, r in test_imp.iterrows():\n",
    "    user_test_gt[int(r[\"uix\"])].append(int(r[\"iix\"]))\n",
    "\n",
    "def eval_recommender(fn_recommend, k=10, max_users=None):\n",
    "    \"\"\"fn_recommend should take a user_id and return a ranked list of ISBNs.\"\"\"\n",
    "    users = list(user_test_gt.keys())\n",
    "    if max_users is not None:\n",
    "        users = users[:max_users]\n",
    "\n",
    "    p, r, n = [], [], []\n",
    "    for uix in users:\n",
    "        uid = ix2uid[uix]  # back to raw user id\n",
    "        gt_iix = user_test_gt[uix]\n",
    "        if not gt_iix:\n",
    "            continue\n",
    "        gt_isbn = [ix2isbn[i] for i in gt_iix]\n",
    "\n",
    "        preds_isbn = fn_recommend(uid)  # just call the provided function\n",
    "        preds_isbn = preds_isbn[:k]\n",
    "\n",
    "        p.append(precision_at_k(preds_isbn, gt_isbn, k))\n",
    "        r.append(recall_at_k(preds_isbn, gt_isbn, k))\n",
    "        n.append(ndcg_at_k(preds_isbn, gt_isbn, k))\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": float(np.mean(p)) if p else 0.0,\n",
    "        f\"Recall@{k}\": float(np.mean(r)) if r else 0.0,\n",
    "        f\"NDCG@{k}\": float(np.mean(n)) if n else 0.0,\n",
    "        \"Users_evaluated\": len(p)\n",
    "    }\n",
    "\n",
    "# Wrap recommenders with chosen N and K once\n",
    "rec_item = lambda uid: recommend_items_for_user(uid, N=50, K=50)\n",
    "rec_user = lambda uid: recommend_by_similar_users(uid, N=50, K=50)\n",
    "\n",
    "# Run evaluation (e.g., k=10)\n",
    "metrics_item = eval_recommender(rec_item, k=10)\n",
    "metrics_user = eval_recommender(rec_user, k=10)\n",
    "\n",
    "print(\"Item-Item metrics:\", metrics_item)\n",
    "print(\"User-User metrics:\", metrics_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recommender(fn_recommend, k=10, max_users=None):\n",
    "    \"\"\"fn_recommend should take a user_id and return a ranked list of ISBNs.\"\"\"\n",
    "    users = list(user_test_gt.keys())\n",
    "    if max_users is not None:\n",
    "        users = users[:max_users]\n",
    "\n",
    "    p, r, n = [], [], []\n",
    "    for uix in users:\n",
    "        uid = ix2uid[uix]  # back to raw user id\n",
    "        gt_iix = user_test_gt[uix]\n",
    "        if not gt_iix:\n",
    "            continue\n",
    "        gt_isbn = [ix2isbn[i] for i in gt_iix]\n",
    "\n",
    "        preds_isbn = fn_recommend(uid)  # just call the provided function\n",
    "        preds_isbn = preds_isbn[:k]\n",
    "\n",
    "        p.append(precision_at_k(preds_isbn, gt_isbn, k))\n",
    "        r.append(recall_at_k(preds_isbn, gt_isbn, k))\n",
    "        n.append(ndcg_at_k(preds_isbn, gt_isbn, k))\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": float(np.mean(p)) if p else 0.0,\n",
    "        f\"Recall@{k}\": float(np.mean(r)) if r else 0.0,\n",
    "        f\"NDCG@{k}\": float(np.mean(n)) if n else 0.0,\n",
    "        \"Users_evaluated\": len(p)\n",
    "    }\n",
    "\n",
    "# Wrap recommenders with chosen N and K once\n",
    "rec_item = lambda uid: recommend_items_for_user(uid, N=50, K=50)\n",
    "rec_user = lambda uid: recommend_by_similar_users(uid, N=50, K=50)\n",
    "\n",
    "# Run evaluation (e.g., k=10)\n",
    "metrics_item = eval_recommender(rec_item, k=10)\n",
    "metrics_user = eval_recommender(rec_user, k=10)\n",
    "\n",
    "print(\"Item-Item metrics:\", metrics_item)\n",
    "print(\"User-User metrics:\", metrics_user)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
