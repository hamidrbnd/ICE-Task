{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJwe8WawFlKC",
        "outputId": "d8cd290a-0e35-421c-a085-d838c38bb0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup & imports"
      ],
      "metadata": {
        "id": "kLvTsaoYF--X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports & config\n",
        "import os, json, random, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---- Repro\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ---- Paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/processed_ice\"                  # where Books.csv, Users.csv, Ratings.csv live\n",
        "OUT_DIR  = \"/content/drive/MyDrive/processed_ice/processed\"        # where we’ll save cleaned files\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZZRjLftF67Z",
        "outputId": "caa97e76-7e38-4843-9a1a-90bf255ac8ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load, clean, map IDs, and build features"
      ],
      "metadata": {
        "id": "w3RxAUhRGHke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load raw CSVs\n",
        "books   = pd.read_csv(f\"{DATA_DIR}/Books.csv\", dtype=str, encoding=\"latin-1\")\n",
        "ratings = pd.read_csv(f\"{DATA_DIR}/Ratings.csv\", dtype=str, encoding=\"latin-1\")\n",
        "users   = pd.read_csv(f\"{DATA_DIR}/Users.csv\", dtype=str, encoding=\"latin-1\")\n",
        "\n",
        "# Normalize column names\n",
        "books.columns   = [c.strip().replace(\" \", \"-\") for c in books.columns]\n",
        "ratings.columns = [c.strip().replace(\" \", \"-\") for c in ratings.columns]\n",
        "users.columns   = [c.strip().replace(\" \", \"-\") for c in users.columns]\n",
        "\n",
        "def to_int_safe(x):\n",
        "    try: return int(float(str(x).strip()))\n",
        "    except: return np.nan\n",
        "\n",
        "def clamp_year(y):\n",
        "    y = to_int_safe(y)\n",
        "    return y if (not pd.isna(y) and 1450 <= y <= 2025) else np.nan\n",
        "\n",
        "def clean_age(a):\n",
        "    a = to_int_safe(a)\n",
        "    return a if (not pd.isna(a) and 5 <= a <= 95) else np.nan\n",
        "\n",
        "def split_location(loc):\n",
        "    if pd.isna(loc): return pd.Series({\"City\": np.nan, \"State\": np.nan, \"Country\": np.nan})\n",
        "    parts = [p.strip().lower() for p in str(loc).split(\",\")]\n",
        "    parts += [np.nan] * (3 - len(parts))\n",
        "    return pd.Series({\"City\": parts[0], \"State\": parts[1], \"Country\": parts[2]})\n",
        "\n",
        "# Clean users\n",
        "users[\"Age\"] = users[\"Age\"].apply(clean_age)\n",
        "loc_split = users[\"Location\"].apply(split_location)\n",
        "users = pd.concat([users.drop(columns=[\"Location\"], errors=\"ignore\"), loc_split], axis=1)\n",
        "\n",
        "bins = [0, 18, 25, 35, 50, 70, 120]\n",
        "labels = [\"<18\",\"18-24\",\"25-34\",\"35-49\",\"50-69\",\"70+\"]\n",
        "users[\"AgeBucket\"] = pd.cut(users[\"Age\"], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# Clean books\n",
        "books[\"Year-Of-Publication\"] = books[\"Year-Of-Publication\"].apply(clamp_year)\n",
        "for c in [\"Book-Title\", \"Book-Author\", \"Publisher\"]:\n",
        "    if c in books.columns:\n",
        "        books[c] = books[c].fillna(\"\").astype(str).str.strip()\n",
        "books = books.drop_duplicates(subset=[\"ISBN\"])\n",
        "\n",
        "# Clean ratings\n",
        "ratings[\"User-ID\"] = ratings[\"User-ID\"].apply(to_int_safe)\n",
        "ratings[\"Book-Rating\"] = ratings[\"Book-Rating\"].apply(to_int_safe)\n",
        "ratings = ratings.dropna(subset=[\"User-ID\",\"ISBN\",\"Book-Rating\"])\n",
        "ratings[\"User-ID\"] = ratings[\"User-ID\"].astype(int)\n",
        "ratings[\"Book-Rating\"] = ratings[\"Book-Rating\"].astype(int)\n",
        "\n",
        "users[\"User-ID\"] = users[\"User-ID\"].astype(int)\n",
        "\n",
        "# Keep only consistent ids across tables\n",
        "df = ratings.merge(books[[\"ISBN\"]], on=\"ISBN\", how=\"inner\")\n",
        "df = df.merge(users[[\"User-ID\"]], on=\"User-ID\", how=\"inner\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XJsKTYG0GQQ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "846CV4qvIqhq",
        "outputId": "5f18db8f-a4bf-48b1-c31a-77fdf9b7c4e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1031136, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: drop \"unnormal raters\" (strange behaviour: all high or low)\n",
        "ustats = df.groupby(\"User-ID\")[\"Book-Rating\"].agg(['count','mean', 'max', 'min']).fillna(0)\n",
        "unnormal_raters = ustats[(ustats['count']>=5) & ((ustats['min']>=9) | (ustats['max']<=1))].index\n",
        "df = df[~df[\"User-ID\"].isin(unnormal_raters)].copy()\n",
        "\n",
        "# Optional: drop \"uniform raters\" (strange behaviour: very low variance)\n",
        "# ustats = df.groupby(\"User-ID\")[\"Book-Rating\"].agg([\"count\",\"std\"]).fillna(0)\n",
        "# uniform_users = ustats[ustats[\"std\"] < 0.5].index\n",
        "# df2 = df[~df[\"User-ID\"].isin(uniform_users)].copy()\n",
        "\n",
        "\n",
        "\n",
        "# Activity filtering for CF (keeps CF stable)\n",
        "MIN_USER_INTERACTIONS = 4\n",
        "MIN_ITEM_INTERACTIONS = 4\n",
        "uc = df[\"User-ID\"].value_counts()\n",
        "ic = df[\"ISBN\"].value_counts()\n",
        "keep_u = set(uc[uc >= MIN_USER_INTERACTIONS].index)\n",
        "keep_i = set(ic[ic >= MIN_ITEM_INTERACTIONS].index)\n",
        "df = df[df[\"User-ID\"].isin(keep_u) & df[\"ISBN\"].isin(keep_i)].copy()"
      ],
      "metadata": {
        "id": "-526TN0RHQ5I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db9C_RuHIw3m",
        "outputId": "6881f0fb-b68e-4820-885e-808d3d061bad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(641240, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Map to contiguous indices\n",
        "uid2ix = {u:i for i,u in enumerate(sorted(df[\"User-ID\"].unique()))}\n",
        "ix2uid = {i:u for u,i in uid2ix.items()}\n",
        "isbn2ix = {b:i for i,b in enumerate(sorted(df[\"ISBN\"].unique()))}\n",
        "ix2isbn = {i:b for b,i in isbn2ix.items()}\n",
        "\n",
        "df[\"uix\"] = df[\"User-ID\"].map(uid2ix)\n",
        "df[\"iix\"] = df[\"ISBN\"].map(isbn2ix)\n",
        "\n",
        "# Implicit view\n",
        "IMPLICIT_THRESH = 5\n",
        "implicit = df.copy()\n",
        "implicit[\"y\"] = (implicit[\"Book-Rating\"] >= IMPLICIT_THRESH).astype(int)\n",
        "\n",
        "# Train/val/test split (leave-one-out per user)\n",
        "def leave_one_out_split(df_in):\n",
        "    df_shuf = df_in.sample(frac=1.0, random_state=SEED)\n",
        "    val_idx, test_idx, seen_v, seen_t = [], [], set(), set()\n",
        "    for idx, row in df_shuf.iterrows():\n",
        "        u = row[\"uix\"]\n",
        "        if u not in seen_v:\n",
        "            val_idx.append(idx); seen_v.add(u)\n",
        "        elif u not in seen_t:\n",
        "            test_idx.append(idx); seen_t.add(u)\n",
        "    val  = df_in.loc[val_idx]\n",
        "    test = df_in.loc[test_idx]\n",
        "    train = df_in.drop(index=set(val_idx) | set(test_idx))\n",
        "    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)\n",
        "\n",
        "train_imp, val_imp, test_imp = leave_one_out_split(implicit[[\"uix\",\"iix\",\"y\",\"User-ID\",\"ISBN\"]])\n",
        "\n",
        "# Build sparse user–item matrix for CF retrieval\n",
        "n_users = len(uid2ix); n_items = len(isbn2ix)\n",
        "rows = train_imp[\"uix\"].to_numpy(); cols = train_imp[\"iix\"].to_numpy(); vals = train_imp[\"y\"].to_numpy()\n",
        "ui_matrix = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
        "\n",
        "# Content features for items (TF-IDF title -> SVD(64))\n",
        "title_series = books.set_index(\"ISBN\").reindex(sorted(isbn2ix.keys()))[\"Book-Title\"].fillna(\"\").astype(str)\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)\n",
        "X_tfidf = tfidf.fit_transform(title_series.tolist())\n",
        "svd = TruncatedSVD(n_components=64, random_state=SEED)\n",
        "item_title_emb = svd.fit_transform(X_tfidf).astype(np.float32)  # [n_items, 64]\n",
        "\n",
        "item_content = pd.DataFrame(item_title_emb, columns=[f\"title_svd_{k}\" for k in range(64)])\n",
        "item_content[\"ISBN\"] = sorted(isbn2ix.keys())\n",
        "item_content[\"iix\"]  = item_content[\"ISBN\"].map(isbn2ix)\n",
        "\n",
        "# User demographic one-hots (country top-K + age bucket)\n",
        "K_COUNTRIES = 15\n",
        "users_small = users[users[\"User-ID\"].isin(uid2ix.keys())].copy()\n",
        "users_small[\"uix\"] = users_small[\"User-ID\"].map(uid2ix)\n",
        "top_c = users_small[\"Country\"].value_counts().head(K_COUNTRIES).index.tolist()\n",
        "for c in top_c: users_small[f\"country__{c}\"] = (users_small[\"Country\"] == c).astype(int)\n",
        "users_small[\"country__other\"] = (~users_small[\"Country\"].isin(top_c)).astype(int)\n",
        "for b in labels: users_small[f\"age__{b}\"] = (users_small[\"AgeBucket\"] == b).astype(int)\n",
        "\n",
        "user_features = users_small[[\"uix\"] + [c for c in users_small.columns if c.startswith((\"country__\",\"age__\"))]].copy()\n",
        "\n",
        "# Dense feature matrices aligned by index\n",
        "item_feat_cols = [c for c in item_content.columns if c.startswith(\"title_svd_\")]\n",
        "I_feat = np.zeros((n_items, len(item_feat_cols)), dtype=np.float32)\n",
        "I_feat[item_content[\"iix\"].values] = item_content[item_feat_cols].values.astype(np.float32)\n",
        "\n",
        "user_feat_cols = [c for c in user_features.columns if c != \"uix\"]\n",
        "U_feat = np.zeros((n_users, len(user_feat_cols)), dtype=np.float32)\n",
        "U_feat[user_features[\"uix\"].values] = user_features[user_feat_cols].values.astype(np.float32)\n",
        "\n",
        "# Save features for inference (also load to torch tensors now)\n",
        "np.save(f\"{OUT_DIR}/I_feat.npy\", I_feat)\n",
        "np.save(f\"{OUT_DIR}/U_feat.npy\", U_feat)\n",
        "I_feat_t = torch.from_numpy(I_feat).to(device)\n",
        "U_feat_t = torch.from_numpy(U_feat).to(device)\n",
        "\n",
        "print(\"Data ready:\",\n",
        "      \"\\n users:\", n_users,\n",
        "      \"\\n items:\", n_items,\n",
        "      \"\\n train/val/test sizes:\", len(train_imp), len(val_imp), len(test_imp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv-RvrNpGIt7",
        "outputId": "8e34b109-30d1-4b96-fd76-b80d9b6b8b55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ready: \n",
            " users: 22978 \n",
            " items: 52775 \n",
            " train/val/test sizes: 595999 22978 22263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "# --- Build User-Item sparse matrix (unchanged)\n",
        "n_users = len(uid2ix)\n",
        "n_items = len(isbn2ix)\n",
        "\n",
        "rows = train_imp[\"uix\"].to_numpy()\n",
        "cols = train_imp[\"iix\"].to_numpy()\n",
        "vals = train_imp[\"y\"].to_numpy()\n",
        "\n",
        "ui_matrix = csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np, heapq\n",
        "\n",
        "# Fit once (items = columns of UI matrix)\n",
        "knn_items = NearestNeighbors(\n",
        "    n_neighbors=50,       # tune K\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1\n",
        ").fit(ui_matrix.T)\n",
        "\n",
        "def recommend_items_for_user(user_id, N=10, K=50):\n",
        "    \"\"\"\n",
        "    On-the-fly item-item recommendations for a user.\n",
        "    Uses only the items the user interacted with, expands to K neighbors each,\n",
        "    and aggregates neighbor similarities.\n",
        "    \"\"\"\n",
        "    if user_id not in uid2ix:\n",
        "        return []\n",
        "\n",
        "    uix = uid2ix[user_id]\n",
        "    user_items = ui_matrix[uix].indices  # items this user interacted with\n",
        "    if len(user_items) == 0:\n",
        "        return []\n",
        "\n",
        "    # Query neighbors for just these items\n",
        "    dist, idx = knn_items.kneighbors(ui_matrix.T[user_items], n_neighbors=K, return_distance=True)\n",
        "    sim = 1.0 - dist   # cosine similarity\n",
        "\n",
        "    # Accumulate scores\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    for nbrs, sims in zip(idx, sim):\n",
        "        # skip self (first neighbor is usually the item itself)\n",
        "        for j, s in zip(nbrs[1:], sims[1:]):\n",
        "            scores[j] += s\n",
        "\n",
        "    # filter items already seen\n",
        "    seen_items = set(user_items.tolist())\n",
        "    if seen_items:\n",
        "        scores[list(seen_items)] = -1e9\n",
        "\n",
        "    top_items = heapq.nlargest(N, range(n_items), key=lambda i: scores[i])\n",
        "    return [ix2isbn[i] for i in top_items]\n",
        "\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np, heapq\n",
        "\n",
        "# Fit once (users = rows of UI matrix)\n",
        "knn_users = NearestNeighbors(\n",
        "    n_neighbors=50,     # tune K\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1\n",
        ").fit(ui_matrix)\n",
        "\n",
        "def recommend_by_similar_users(user_id, N=10, K=50):\n",
        "    \"\"\"\n",
        "    User-user collaborative filtering.\n",
        "    Finds K nearest users to the target user and aggregates their interactions\n",
        "    weighted by similarity, all in sparse form (no dense matrix conversion).\n",
        "    \"\"\"\n",
        "    if user_id not in uid2ix:\n",
        "        return []\n",
        "\n",
        "    uix = uid2ix[user_id]\n",
        "\n",
        "    # Find K nearest neighbors for this user (includes self at idx 0)\n",
        "    dist, idx = knn_users.kneighbors(ui_matrix[uix], n_neighbors=K, return_distance=True)\n",
        "    idx = idx.ravel()\n",
        "    sim = (1.0 - dist.ravel()).astype(np.float32)\n",
        "\n",
        "    # drop self if present\n",
        "    mask = idx != uix\n",
        "    nbr_idxs = idx[mask]\n",
        "    nbr_sims = sim[mask]\n",
        "\n",
        "    # Accumulate scores sparsely: scores[j] += w * interaction(nbr, j)\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    for nbr, w in zip(nbr_idxs, nbr_sims):\n",
        "        row = ui_matrix[nbr]           # sparse row\n",
        "        if row.nnz:\n",
        "            scores[row.indices] += w * row.data\n",
        "\n",
        "    # filter items already seen by target user\n",
        "    seen_items = set(ui_matrix[uix].indices.tolist())\n",
        "    if seen_items:\n",
        "        scores[list(seen_items)] = -1e9\n",
        "\n",
        "    top_items = heapq.nlargest(N, range(n_items), key=lambda i: scores[i])\n",
        "    return [ix2isbn[i] for i in top_items]\n",
        "\n",
        "\n",
        "# Pick any known user_id from your mappings\n",
        "some_user_id = next(iter(uid2ix.keys()))\n",
        "\n",
        "dict_book = books[['ISBN', 'Book-Title']].set_index('ISBN').to_dict()['Book-Title']\n",
        "\n",
        "recommended_item_itembased = recommend_items_for_user(some_user_id, N=5, K=50)\n",
        "recommended_item_userbased  = recommend_by_similar_users(some_user_id, N=5, K=50)\n",
        "\n",
        "print(\"Item-Item recs:\", recommended_item_itembased)\n",
        "print(\"User-User recs:\", recommended_item_userbased)\n",
        "[dict_book[x] for x in recommended_item_userbased]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKY9eXSMKK1d",
        "outputId": "c4cef6a1-d422-4f7a-95e0-963a802ca1e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item-Item recs: ['0786863269', '1558744673', '0373169663', '0373225946', '0771095066']\n",
            "User-User recs: ['0020811853', '0316569321', '038082101X', '0316601950', '0440214009']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['POSTCARDS',\n",
              " 'White Oleander : A Novel',\n",
              " 'Daughter of Fortune: A Novel',\n",
              " \"The Pilot's Wife : A Novel\",\n",
              " 'Treasures']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dual"
      ],
      "metadata": {
        "id": "nUpkFZ1C2E43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "# Build once (analogous to knn_items for columns)\n",
        "K_USER_DEFAULT = 100\n",
        "knn_users = NearestNeighbors(\n",
        "    n_neighbors=K_USER_DEFAULT,\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1\n",
        ").fit(ui_matrix)   # users are rows\n",
        "\n",
        "\n",
        "def item_knn_scores_for_user(uix: int, K: int = 50) -> np.ndarray:\n",
        "    \"\"\"Return a dense score vector over items for one user using item-item CF.\"\"\"\n",
        "    user_items = ui_matrix[uix].indices\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    if len(user_items) == 0:\n",
        "        return scores\n",
        "\n",
        "    # neighbors for the items the user interacted with\n",
        "    dist, idx = knn_items.kneighbors(ui_matrix.T[user_items], n_neighbors=K, return_distance=True)\n",
        "    sim = 1.0 - dist\n",
        "\n",
        "    for nbrs, sims in zip(idx, sim):\n",
        "        for j, s in zip(nbrs[1:], sims[1:]):  # skip self\n",
        "            scores[j] += s\n",
        "    return scores\n",
        "\n",
        "def user_knn_scores_for_user(uix: int, K: int = 100) -> np.ndarray:\n",
        "    \"\"\"Return a dense score vector over items for one user using user-user CF.\"\"\"\n",
        "    # find K most similar users to uix\n",
        "    dist, idx = knn_users.kneighbors(ui_matrix[uix], n_neighbors=K, return_distance=True)\n",
        "    sim_users = 1.0 - dist.ravel()\n",
        "    nbr_users = idx.ravel()\n",
        "\n",
        "    # aggregate neighbor preference weighted by similarity\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    for s, v in zip(sim_users[1:], nbr_users[1:]):  # skip self\n",
        "        if s <= 0:\n",
        "            continue\n",
        "        # items that neighbor v interacted with\n",
        "        v_items = ui_matrix[v].indices\n",
        "        scores[v_items] += s\n",
        "    return scores\n",
        "\n",
        "\n",
        "def dual_pool_candidates(\n",
        "    user_id,\n",
        "    pool_item: int = 200,\n",
        "    pool_user: int = 200,\n",
        "    K_item: int = 50,\n",
        "    K_user: int = 100,\n",
        "    w_item: float = 1.0,\n",
        "    w_user: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Produce a merged candidate list from item-item CF and user-user CF.\n",
        "    Returns a list of ISBNs ordered by combined score.\n",
        "    \"\"\"\n",
        "    if user_id not in uid2ix:\n",
        "        return []\n",
        "    uix = uid2ix[user_id]\n",
        "\n",
        "    # 1) get dense score vectors from both CFs\n",
        "    s_item = item_knn_scores_for_user(uix, K=K_item)   # shape [n_items]\n",
        "    s_user = user_knn_scores_for_user(uix, K=K_user)   # shape [n_items]\n",
        "\n",
        "    # 2) mask items the user already saw\n",
        "    seen = set(ui_matrix[uix].indices.tolist())\n",
        "    if seen:\n",
        "        s_item[list(seen)] = -1e9\n",
        "        s_user[list(seen)] = -1e9\n",
        "\n",
        "    # 3) take top pools from each scorer\n",
        "    cand_item_idx = np.argsort(-s_item)[:pool_item]\n",
        "    cand_user_idx = np.argsort(-s_user)[:pool_user]\n",
        "\n",
        "    # 4) merge with weighted sum\n",
        "    merged = {}\n",
        "    for i in cand_item_idx:\n",
        "        if s_item[i] > -1e8:     # not masked\n",
        "            merged[i] = merged.get(i, 0.0) + w_item * float(s_item[i])\n",
        "    for i in cand_user_idx:\n",
        "        if s_user[i] > -1e8:\n",
        "            merged[i] = merged.get(i, 0.0) + w_user * float(s_user[i])\n",
        "\n",
        "    if not merged:\n",
        "        return []\n",
        "\n",
        "    # 5) rank by combined score\n",
        "    ranked_iix = sorted(merged.keys(), key=lambda j: merged[j], reverse=True)\n",
        "    return [ix2isbn[i] for i in ranked_iix]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def rerank_hybrid_dualpool(user_id, N_final=10, pool_item=200, pool_user=200, K_item=50, K_user=100, w_item=1.0, w_user=1.0):\n",
        "    # 1) dual candidates\n",
        "    cands_isbn = dual_pool_candidates(\n",
        "        user_id,\n",
        "        pool_item=pool_item, pool_user=pool_user,\n",
        "        K_item=K_item, K_user=K_user,\n",
        "        w_item=w_item, w_user=w_user\n",
        "    )\n",
        "    if not cands_isbn:\n",
        "        return []\n",
        "\n",
        "    # 2) re-rank with Hybrid (same as before)\n",
        "    cands_iix = torch.tensor([isbn2ix[i] for i in cands_isbn], dtype=torch.long, device=device)\n",
        "    uix = torch.tensor([uid2ix[user_id]], dtype=torch.long, device=device)\n",
        "    logits = infer_model(uix, cands_iix, U_feat=U_feat_inf, I_feat=I_feat_inf).float().cpu().numpy()\n",
        "    order = np.argsort(-logits)[:N_final].tolist()\n",
        "    return [cands_isbn[i] for i in order]\n",
        "\n",
        "\n",
        "some_user = next(iter(uid2ix.keys()))\n",
        "print(\"Dual-pool Hybrid Top-10:\", rerank_hybrid_dualpool(some_user, N_final=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5rAY2uj2AfE",
        "outputId": "a8caf82c-968a-4dbf-eb32-fc7611304811"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dual-pool Hybrid Top-10: ['0971880107', '0142001740', '0316666343', '0060928336', '0671027360', '067976402X', '0553375407', '0312195516', '1400034779', '0140119906']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CF Retrieval (Item–Item KNN, Method-B on-the-fly)"
      ],
      "metadata": {
        "id": "4aMyFqZzJD9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dataset with on-the-fly negatives (implicit)\n",
        "class ImplicitPairDataset(Dataset):\n",
        "    def __init__(self, df_pos, n_items, num_neg=4, seed=42):\n",
        "        self.df_pos = df_pos[[\"uix\",\"iix\"]].drop_duplicates().reset_index(drop=True)\n",
        "        self.n_items = n_items\n",
        "        self.num_neg = num_neg\n",
        "        self.rng = random.Random(seed)\n",
        "        self.user_pos = defaultdict(set)\n",
        "        for u, i in self.df_pos[[\"uix\",\"iix\"]].itertuples(index=False):\n",
        "            self.user_pos[int(u)].add(int(i))\n",
        "\n",
        "    def __len__(self): return len(self.df_pos)\n",
        "\n",
        "    def _sample_neg(self, u):\n",
        "        while True:\n",
        "            j = self.rng.randint(0, self.n_items-1)\n",
        "            if j not in self.user_pos[u]:\n",
        "                return j\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        u, i_pos = map(int, self.df_pos.iloc[idx].values)\n",
        "        items = [i_pos] + [self._sample_neg(u) for _ in range(self.num_neg)]\n",
        "        labels = [1] + [0]*self.num_neg\n",
        "        return int(u), torch.tensor(items, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# === Model\n",
        "class HybridRec(nn.Module):\n",
        "    def __init__(self, n_users, n_items, d_id=64, d_user_feat=0, d_item_feat=64, hidden=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, d_id)\n",
        "        self.item_emb = nn.Embedding(n_items, d_id)\n",
        "        self.use_user_feat = d_user_feat > 0\n",
        "        self.use_item_feat = d_item_feat > 0\n",
        "        if self.use_user_feat: self.user_feat_proj = nn.Linear(d_user_feat, d_id)\n",
        "        if self.use_item_feat: self.item_feat_proj = nn.Linear(d_item_feat, d_id)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_id*2, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden//2, 1)\n",
        "        )\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.02)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.02)\n",
        "\n",
        "    def forward(self, uix, iix, U_feat=None, I_feat=None):\n",
        "        # Inference path: single user vs (M,) items\n",
        "        if iix.ndim == 1:\n",
        "            uix = uix.repeat(iix.shape[0])     # (M,)\n",
        "            u = self.user_emb(uix)             # (M,d)\n",
        "            i = self.item_emb(iix)             # (M,d)\n",
        "            if self.use_user_feat and U_feat is not None: u = u + self.user_feat_proj(U_feat[uix])\n",
        "            if self.use_item_feat and I_feat is not None: i = i + self.item_feat_proj(I_feat[iix])\n",
        "            x = torch.cat([u, i], dim=-1)      # (M,2d)\n",
        "            return self.mlp(x).squeeze(-1)     # (M,)\n",
        "\n",
        "        # Training path: (B,M)\n",
        "        B, M = iix.shape\n",
        "        u = self.user_emb(uix)                 # (B,d)\n",
        "        i = self.item_emb(iix)                 # (B,M,d)\n",
        "        if self.use_user_feat and U_feat is not None: u = u + self.user_feat_proj(U_feat[uix])\n",
        "        if self.use_item_feat and I_feat is not None: i = i + self.item_feat_proj(I_feat[iix])\n",
        "        u = u.unsqueeze(1).expand(-1, M, -1)   # (B,M,d)\n",
        "        x = torch.cat([u, i], dim=-1)          # (B,M,2d)\n",
        "        return self.mlp(x).squeeze(-1)         # (B,M)\n",
        "\n",
        "# === Early stopping helper\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=4, min_delta=1e-4, mode=\"min\"):\n",
        "        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n",
        "        self.best, self.bad_epochs, self.should_stop = None, 0, False\n",
        "    def step(self, value):\n",
        "        if self.best is None: self.best = value; return False\n",
        "        improve = (value < self.best - self.min_delta) if self.mode==\"min\" else (value > self.best + self.min_delta)\n",
        "        if improve: self.best = value; self.bad_epochs = 0; return False\n",
        "        self.bad_epochs += 1\n",
        "        if self.bad_epochs >= self.patience: self.should_stop = True\n",
        "        return self.should_stop\n",
        "\n",
        "# === DataLoaders\n",
        "num_neg = 4; batch_size = 256\n",
        "train_ds = ImplicitPairDataset(train_imp, n_items, num_neg=num_neg)\n",
        "val_ds   = ImplicitPairDataset(val_imp,   n_items, num_neg=num_neg*2)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, drop_last=False)\n",
        "\n",
        "# === Train with BCE + early stopping; save best\n",
        "model = HybridRec(n_users, n_items, d_id=64, d_user_feat=U_feat_t.shape[1], d_item_feat=I_feat_t.shape[1],\n",
        "                  hidden=128, dropout=0.1).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "CKPT_PATH = f\"{OUT_DIR}/hybrid_model.pt\"\n",
        "CFG_PATH  = f\"{OUT_DIR}/hybrid_config.json\"\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(mode=train)\n",
        "    total, n = 0.0, 0\n",
        "    for uix, items, labels in loader:\n",
        "        uix, items, labels = uix.to(device), items.to(device), labels.to(device)\n",
        "        logits = model(uix, items, U_feat=U_feat_t, I_feat=I_feat_t)  # (B,1+num_neg)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "            opt.step()\n",
        "        total += loss.item() * uix.size(0); n += uix.size(0)\n",
        "    return total / max(n,1)\n",
        "\n",
        "epochs = 30\n",
        "early = EarlyStopping(patience=4, min_delta=1e-4, mode=\"min\")\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "    tr = run_epoch(train_loader, train=True)\n",
        "    vl = run_epoch(val_loader,   train=False)\n",
        "    print(f\"epoch {ep:02d} | train_loss {tr:.4f} | val_loss {vl:.4f}\")\n",
        "\n",
        "    if vl < best_val - 1e-4:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), CKPT_PATH)\n",
        "        with open(CFG_PATH, \"w\") as f:\n",
        "            json.dump({\n",
        "                \"n_users\": n_users, \"n_items\": n_items,\n",
        "                \"d_id\": 64, \"d_user_feat\": int(U_feat_t.shape[1]), \"d_item_feat\": int(I_feat_t.shape[1]),\n",
        "                \"hidden\": 128, \"dropout\": 0.1\n",
        "            }, f)\n",
        "        print(f\"  ✓ saved best to {CKPT_PATH}\")\n",
        "\n",
        "    if early.step(vl):\n",
        "        print(f\"Early stopping at epoch {ep} (best val_loss={early.best:.4f})\")\n",
        "        break\n",
        "\n",
        "print(\"Training done. Best val_loss:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0p-odVgGG8Z",
        "outputId": "0694d0fa-8d28-4315-a9a0-a4a2204c05bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train_loss 0.4162 | val_loss 0.2866\n",
            "  ✓ saved best to /content/drive/MyDrive/processed_ice/processed/hybrid_model.pt\n",
            "epoch 02 | train_loss 0.3607 | val_loss 0.2700\n",
            "  ✓ saved best to /content/drive/MyDrive/processed_ice/processed/hybrid_model.pt\n",
            "epoch 03 | train_loss 0.3307 | val_loss 0.2649\n",
            "  ✓ saved best to /content/drive/MyDrive/processed_ice/processed/hybrid_model.pt\n",
            "epoch 04 | train_loss 0.3081 | val_loss 0.2646\n",
            "  ✓ saved best to /content/drive/MyDrive/processed_ice/processed/hybrid_model.pt\n",
            "epoch 05 | train_loss 0.2885 | val_loss 0.2705\n",
            "epoch 06 | train_loss 0.2712 | val_loss 0.2773\n",
            "epoch 07 | train_loss 0.2555 | val_loss 0.2904\n",
            "epoch 08 | train_loss 0.2414 | val_loss 0.3157\n",
            "Early stopping at epoch 8 (best val_loss=0.2646)\n",
            "Training done. Best val_loss: 0.26457403995409906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics (Precision@K, Recall@K, NDCG@K)"
      ],
      "metadata": {
        "id": "zg6yy4wbLCVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(pred, gt, k):\n",
        "    pred_k = pred[:k]\n",
        "    return len(set(pred_k) & set(gt)) / float(k) if k > 0 else 0.0\n",
        "\n",
        "def recall_at_k(pred, gt, k):\n",
        "    if len(gt) == 0: return 0.0\n",
        "    pred_k = pred[:k]\n",
        "    return len(set(pred_k) & set(gt)) / float(len(gt))\n",
        "\n",
        "def ndcg_at_k(pred, gt, k):\n",
        "    pred_k = pred[:k]\n",
        "    if not pred_k: return 0.0\n",
        "    gains = [1.0 if p in gt else 0.0 for p in pred_k]\n",
        "    dcg = sum(g / np.log2(i + 2) for i, g in enumerate(gains))\n",
        "    ideal_gains = sorted(gains, reverse=True)\n",
        "    idcg = sum(g / np.log2(i + 2) for i, g in enumerate(ideal_gains))\n",
        "    return (dcg / idcg) if idcg > 0 else 0.0\n",
        "\n",
        "# Ground truth from TEST split (implicit)\n",
        "user_test_gt = defaultdict(list)\n",
        "for _, r in test_imp.iterrows():\n",
        "    user_test_gt[int(r[\"uix\"])].append(int(r[\"iix\"]))\n",
        "\n",
        "def eval_recommender(fn_recommend, k=10, max_users=None):\n",
        "    users_eval = list(user_test_gt.keys())\n",
        "    if max_users: users_eval = users_eval[:max_users]\n",
        "    P, R, N = [], [], []\n",
        "    for uix in users_eval:\n",
        "        uid = ix2uid[uix]\n",
        "        gt_isbn = [ix2isbn[i] for i in user_test_gt[uix]]\n",
        "        preds = fn_recommend(uid)\n",
        "        preds = preds[:k]\n",
        "        P.append(precision_at_k(preds, gt_isbn, k))\n",
        "        R.append(recall_at_k(preds, gt_isbn, k))\n",
        "        N.append(ndcg_at_k(preds, gt_isbn, k))\n",
        "    return {f\"Precision@{k}\": float(np.mean(P) if P else 0.0),\n",
        "            f\"Recall@{k}\": float(np.mean(R) if R else 0.0),\n",
        "            f\"NDCG@{k}\": float(np.mean(N) if N else 0.0),\n",
        "            \"Users_evaluated\": len(P)}\n"
      ],
      "metadata": {
        "id": "PqqSsU-MK4rp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference: load best checkpoint and re-rank candidates"
      ],
      "metadata": {
        "id": "DMxSvrHHK82S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load best model for inference\n",
        "with open(CFG_PATH, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "infer_model = HybridRec(\n",
        "    n_users=cfg[\"n_users\"], n_items=cfg[\"n_items\"],\n",
        "    d_id=cfg[\"d_id\"], d_user_feat=cfg[\"d_user_feat\"], d_item_feat=cfg[\"d_item_feat\"],\n",
        "    hidden=cfg[\"hidden\"], dropout=cfg[\"dropout\"]\n",
        ").to(device).eval()\n",
        "\n",
        "state = torch.load(CKPT_PATH, map_location=device)\n",
        "infer_model.load_state_dict(state)\n",
        "\n",
        "U_feat_inf = torch.from_numpy(np.load(f\"{OUT_DIR}/U_feat.npy\")).to(device)\n",
        "I_feat_inf = torch.from_numpy(np.load(f\"{OUT_DIR}/I_feat.npy\")).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def rerank_with_model(user_id, N_final=10, cand_K=50, pool=200):\n",
        "    \"\"\"Candidate retrieval (item-KNN) + Hybrid re-ranking.\"\"\"\n",
        "    if user_id not in uid2ix: return []\n",
        "    # 1) candidates\n",
        "    cands_isbn = recommend_items_for_user(user_id, N=max(pool, N_final*5), K=cand_K)\n",
        "    if not cands_isbn: return []\n",
        "    # 2) re-rank\n",
        "    cands_iix = torch.tensor([isbn2ix[i] for i in cands_isbn], dtype=torch.long, device=device)\n",
        "    uix = torch.tensor([uid2ix[user_id]], dtype=torch.long, device=device)\n",
        "    logits = infer_model(uix, cands_iix, U_feat=U_feat_inf, I_feat=I_feat_inf).float().cpu().numpy()\n",
        "    order = np.argsort(-logits)[:N_final].tolist()\n",
        "    return [cands_isbn[i] for i in order]\n",
        "\n",
        "# === Quick smoke test\n",
        "some_user_id = next(iter(uid2ix.keys()))\n",
        "print(\"Collaborative Filtering item base:\", recommend_items_for_user(some_user_id, N=5, K=50))\n",
        "print(\"Collaborative Filtering user base:\", recommend_by_similar_users(some_user_id, N=5, K=50))\n",
        "print(\"Hybrid re-ranked:\", rerank_with_model(some_user_id, N_final=5, cand_K=50, pool=200))\n",
        "print(\"Dual-pool Hybrid:\", rerank_hybrid_dualpool(some_user, N_final=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikcEVOR9K6tu",
        "outputId": "53683334-d52f-4f82-afb6-771dd52b5c00"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collaborative Filtering item base: ['0786863269', '1558744673', '0373169663', '0373225946', '0771095066']\n",
            "Collaborative Filtering user base: ['0020811853', '0316569321', '038082101X', '0316601950', '0440214009']\n",
            "Hybrid re-ranked: ['0020811853', '0609600761', '0553263633', '0375704299', '0006375952']\n",
            "Dual-pool Hybrid: ['0971880107', '0142001740', '0316666343', '0060928336', '0671027360']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate CF vs Hybrid"
      ],
      "metadata": {
        "id": "vl_g1WmeLPnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap recommenders\n",
        "rec_item = lambda uid: recommend_items_for_user(uid, N=100, K=50)\n",
        "rec_user = lambda uid: recommend_by_similar_users(uid, N=100, K=50)\n",
        "rec_hybrid = lambda uid: rerank_with_model(uid, N_final=50, cand_K=50, pool=200)\n",
        "\n",
        "print(\"ItemBased-KNN @10:\", eval_recommender(rec_item,   k=10, max_users=5000))\n",
        "print(\"UserBased-KNN @10:\", eval_recommender(rec_user,   k=10, max_users=5000))\n",
        "print(\"Hybrid    @10:\", eval_recommender(rec_hybrid, k=10, max_users=5000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTItGb4GLNpt",
        "outputId": "b93d9f69-86c7-493d-e71b-5d8140828f12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ItemBased-KNN @10: {'Precision@10': 0.00152, 'Recall@10': 0.0152, 'NDCG@10': 0.009296202432033643, 'Users_evaluated': 5000}\n",
            "UserBased-KNN @10: {'Precision@10': 0.00296, 'Recall@10': 0.0296, 'NDCG@10': 0.018920277996271637, 'Users_evaluated': 5000}\n",
            "Hybrid    @10: {'Precision@10': 0.00244, 'Recall@10': 0.0244, 'NDCG@10': 0.014179776910649622, 'Users_evaluated': 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate @10 (same eval_recommender you already have)\n",
        "rec_dual_hybrid = lambda uid: rerank_hybrid_dualpool(uid, N_final=50, pool_item=150, pool_user=150, K_item=50, K_user=100, w_item=1.0, w_user=1.0)\n",
        "print(\"Dual-pool Hybrid @10:\", eval_recommender(rec_dual_hybrid, k=10, max_users=5000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osrYB0rVrWcu",
        "outputId": "c6fa59fc-4433-403a-c523-049458501de9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dual-pool Hybrid @10: {'Precision@10': 0.00224, 'Recall@10': 0.0224, 'NDCG@10': 0.011551350467791734, 'Users_evaluated': 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp9dXvaCOdqM",
        "outputId": "8133f973-6a85-4d70-9855-c8b9881ae5e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://6538e19ccce0104bba.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3158, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gradio_app.py\", line 176, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3055, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3162, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1153, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6538e19ccce0104bba.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "Zvop7Mcx2aSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Install if needed: pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# ---------- Helpers to build features ----------\n",
        "\n",
        "# (A) Precompute handy stats\n",
        "user_pos_items = {}  # uix -> set of positive item indices (train)\n",
        "for u,i in train_imp[[\"uix\",\"iix\"]].itertuples(index=False):\n",
        "    user_pos_items.setdefault(int(u), set()).add(int(i))\n",
        "\n",
        "item_pop = train_imp[\"iix\"].value_counts().to_dict()\n",
        "user_act = train_imp[\"uix\"].value_counts().to_dict()\n",
        "\n",
        "def user_profile_vector(uix):\n",
        "    \"\"\"Mean of item content vectors for user's positive items.\"\"\"\n",
        "    items = list(user_pos_items.get(int(uix), []))\n",
        "    if not items:\n",
        "        return np.zeros(I_feat.shape[1], dtype=np.float32)\n",
        "    return I_feat[items].mean(axis=0)\n",
        "\n",
        "# cache user profiles\n",
        "U_prof = np.vstack([user_profile_vector(u) for u in range(n_users)])  # [n_users, d]\n",
        "\n",
        "def cosine(a, b, eps=1e-9):\n",
        "    denom = (np.linalg.norm(a)+eps) * (np.linalg.norm(b)+eps)\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "# (B) Item–item KNN score for a candidate list (reuse your knn and ui_matrix)\n",
        "def knn_item_score_for_user_candidates(uix, cand_iix, K=50):\n",
        "    \"\"\"Aggregate neighbor sims for each candidate (like retrieval) but returned as a feature.\"\"\"\n",
        "    user_items = ui_matrix[uix].indices\n",
        "    if len(user_items) == 0:\n",
        "        return np.zeros(len(cand_iix), dtype=np.float32)\n",
        "    # neighbors for user's items\n",
        "    dist, idx = knn_items.kneighbors(ui_matrix.T[user_items], n_neighbors=K, return_distance=True)\n",
        "    sim = 1.0 - dist\n",
        "    # accumulate into a dense score vector, then pick only candidate entries\n",
        "    scores = np.zeros(n_items, dtype=np.float32)\n",
        "    for nbrs, sims in zip(idx, sim):\n",
        "        for j, s in zip(nbrs[1:], sims[1:]):\n",
        "            scores[j] += s\n",
        "    return scores[cand_iix]\n",
        "\n",
        "# (C) Build feature rows for pairs\n",
        "def build_pair_features(uix, iix_list):\n",
        "    # arrays to fill\n",
        "    feats = []\n",
        "    # prefetch\n",
        "    up = U_prof[uix]\n",
        "    u_demo = U_feat[uix]  # demographics one-hot vector\n",
        "    # KNN aggregation for these candidates\n",
        "    knn_scores = knn_item_score_for_user_candidates(uix, np.array(iix_list, dtype=int), K=50)\n",
        "\n",
        "    for k, iix in enumerate(iix_list):\n",
        "        ivec = I_feat[iix]\n",
        "        feats.append([\n",
        "            # collaborative stats\n",
        "            user_act.get(int(uix), 0),\n",
        "            item_pop.get(int(iix), 0),\n",
        "            knn_scores[k],\n",
        "            # content match\n",
        "            cosine(up, ivec),\n",
        "            # cheap metadata proxies\n",
        "            # you can append author/publisher one-hots if you made them\n",
        "        ] + u_demo.tolist()  # append demographics\n",
        "        )\n",
        "    return np.asarray(feats, dtype=np.float32)\n",
        "\n",
        "# Column names (for debugging/importance)\n",
        "base_cols = [\"user_activity\", \"item_popularity\", \"knn_item_score\", \"cos_userprof_item\"]\n",
        "demo_cols = [c for c in user_features.columns if c != \"uix\"]\n",
        "feature_names = base_cols + demo_cols\n"
      ],
      "metadata": {
        "id": "1tJAHv0uqCP_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def sample_negatives(uix, num_neg=4):\n",
        "    pos = user_pos_items.get(int(uix), set())\n",
        "    out = []\n",
        "    while len(out) < num_neg:\n",
        "        j = random.randint(0, n_items - 1)\n",
        "        if j not in pos:\n",
        "            out.append(j)\n",
        "    return out\n",
        "\n",
        "X_list, y_list, qid_list = [], [], []  # qid optional for pointwise (ignored)\n",
        "\n",
        "for uix in range(n_users):\n",
        "    pos_items = list(user_pos_items.get(uix, []))\n",
        "    for iix in pos_items:\n",
        "        # positive\n",
        "        X_list.append(build_pair_features(uix, [iix])[0]); y_list.append(1); qid_list.append(uix)\n",
        "        # negatives\n",
        "        negs = sample_negatives(uix, num_neg=4)\n",
        "        X_list.append(build_pair_features(uix, negs))  # shape [4, F]\n",
        "        y_list += [0]*len(negs)\n",
        "        qid_list += [uix]*len(negs)\n",
        "\n",
        "X = np.vstack([x if x.ndim==2 else np.expand_dims(x,0) for x in X_list])\n",
        "y = np.array(y_list, dtype=np.float32)\n",
        "dtrain = xgb.DMatrix(X, label=y, feature_names=feature_names)\n",
        "\n",
        "# Validation set (build the same way with val_imp)\n",
        "# (for brevity, you can skip or do a smaller sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YoN6fubXqFQw",
        "outputId": "c6aff5bc-4989-48cf-9dee-37a55268f1ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4266345663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnegs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_negatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_pair_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape [4, F]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0my_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mqid_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3602983531.py\u001b[0m in \u001b[0;36mbuild_pair_features\u001b[0;34m(uix, iix_list)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mu_demo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# demographics one-hot vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# KNN aggregation for these candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mknn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_item_score_for_user_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miix_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miix_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3602983531.py\u001b[0m in \u001b[0;36mknn_item_score_for_user_candidates\u001b[0;34m(uix, cand_iix, K)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_iix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# neighbors for user's items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mui_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# accumulate into a dense score vector, then pick only candidate entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             chunked_results = list(\n\u001b[0m\u001b[1;32m    907\u001b[0m                 pairwise_distances_chunked(\n\u001b[1;32m    908\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m             \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m             \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m             \u001b[0m_check_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mD_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_kneighbors_reduce_func\u001b[0;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[1;32m    749\u001b[0m         \u001b[0msample_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;31m# argpartition doesn't guarantee sorted order, so we sort again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \"\"\"\n\u001b[0;32m--> 908\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argpartition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_rows, y_rows, group = [], [], []\n",
        "for uix in range(n_users):\n",
        "    pos_items = list(user_pos_items.get(uix, []))\n",
        "    if not pos_items:\n",
        "        continue\n",
        "    # sample limited positives to keep dataset size reasonable\n",
        "    pos_items = pos_items[:5]\n",
        "    for iix in pos_items:\n",
        "        negs = sample_negatives(uix, num_neg=4)\n",
        "        items = [iix] + negs\n",
        "        feats = build_pair_features(uix, items)\n",
        "        labels = [1] + [0]*len(negs)\n",
        "        X_rows.append(feats); y_rows.append(labels); group.append(len(items))\n",
        "\n",
        "X = np.vstack(X_rows)              # [sum_group, F]\n",
        "y = np.hstack(y_rows).astype(np.float32)\n",
        "dtrain = xgb.DMatrix(X, label=y, feature_names=feature_names)\n",
        "dtrain.set_group(group)\n",
        "\n",
        "# (Optional) Build dvalid in the same way from val_imp and set_group too.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Vvzf3cqEqL9s",
        "outputId": "361266a1-f175-4378-e844-af45eef818ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2878230998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnegs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_negatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_pair_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0my_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3602983531.py\u001b[0m in \u001b[0;36mbuild_pair_features\u001b[0;34m(uix, iix_list)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mu_demo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# demographics one-hot vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# KNN aggregation for these candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mknn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_item_score_for_user_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miix_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miix_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3602983531.py\u001b[0m in \u001b[0;36mknn_item_score_for_user_candidates\u001b[0;34m(uix, cand_iix, K)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_iix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# neighbors for user's items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mui_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# accumulate into a dense score vector, then pick only candidate entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m# arrayXarray preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             elif (row.ndim == 2 and row.shape[1] == 1\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_csc.py\u001b[0m in \u001b[0;36m_get_arrayXslice\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_arrayXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minor_index_fancy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# these functions are used by the parent class (_cs_matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m_minor_index_fancy\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mres_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mres_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         csr_column_index2(col_order, col_offsets, len(self.indices),\n\u001b[0m\u001b[1;32m    683\u001b[0m                           indices, self.data, res_indices, res_data)\n\u001b[1;32m    684\u001b[0m         return self.__class__((res_data, res_indices, res_indptr),\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = dict(\n",
        "    max_depth=6,\n",
        "    eta=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"ndcg@10\",  # good for ranking; for logistic use \"logloss\" or \"auc\"\n",
        ")\n",
        "\n",
        "# For pointwise:\n",
        "# params[\"objective\"] = \"binary:logistic\"\n",
        "\n",
        "# For pairwise:\n",
        "params[\"objective\"] = \"rank:pairwise\"\n",
        "\n",
        "# Fit\n",
        "# If you have dvalid:\n",
        "# watchlist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
        "watchlist = [(dtrain, \"train\")]\n",
        "booster = xgb.train(params, dtrain, num_boost_round=300, evals=watchlist, early_stopping_rounds=30)\n",
        "\n",
        "# Save\n",
        "xgb_path = f\"{OUT_DIR}/xgb_ranker.json\"\n",
        "booster.save_model(xgb_path)\n"
      ],
      "metadata": {
        "id": "f0PzZtTfqOvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "xgb_ranker = xgb.Booster()\n",
        "xgb_ranker.load_model(f\"{OUT_DIR}/xgb_ranker.json\")\n",
        "\n",
        "def rerank_with_xgb(user_id, N_final=10, cand_K=50, pool=200):\n",
        "    if user_id not in uid2ix:\n",
        "        return []\n",
        "    uix = uid2ix[user_id]\n",
        "    # 1) candidate pool\n",
        "    cands_isbn = recommend_items_for_user(user_id, N=pool, K=cand_K)\n",
        "    if not cands_isbn:\n",
        "        return []\n",
        "    cands_iix = [isbn2ix[i] for i in cands_isbn]\n",
        "    # 2) features\n",
        "    X = build_pair_features(uix, cands_iix)\n",
        "    dtest = xgb.DMatrix(X, feature_names=feature_names)\n",
        "    scores = xgb_ranker.predict(dtest)\n",
        "    order = np.argsort(-scores)[:N_final]\n",
        "    return [cands_isbn[i] for i in order]\n"
      ],
      "metadata": {
        "id": "kXAUKZDuqQmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_xgb = lambda uid: rerank_with_xgb(uid, N_final=50, cand_K=50, pool=200)\n",
        "print(\"XGBoost @10:\", eval_recommender(rec_xgb, k=10, max_users=5000))\n"
      ],
      "metadata": {
        "id": "v803PHTYwcT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}